Author：Ciaosu Lee

本文是个人学习自然语言处理的简要笔记，主要是作爲个人思路梳理的作用。
参考资料：
* https://www.cnblogs.com/pinard/


### **特徵工程**

* 在数据分析中更多的是经验和权衡。
* 分爲三个部分：
   * 特徵选择
      * 关係到机器学习算法的上限。
      * 原则：尽量不错过一个可能有用的特徵，但也不滥用太多特徵。
   * 特徵表达
      * 对某一特徵的具体表现形式作处理。
   * 特徵预处理
      * 主要包括特征的归一化和标准化，异常特征样本清洗与样本数据不平衡问题的处理。

#### 特徵选择

* 特徵的来源
   * 从业务已经整理好的各种特徵数据中找到适合我们问题需要的特徵；
   * 我们从业务特徵中自己去寻找高级数据特徵。
   
* 选择合适的特徵
1. 找到领域专家来得到建议，得到第一候选集。
2. 在降维之前用特徵工程的方法选择出较重要的特徵集合。（不会用到领域知识。）
   * 最简单的方法：方差筛选。
      * 方差越大的特徵，认爲其较有用。
      * 在实际中指定一个方差的阈值，筛掉方差小于这个阈值的特徵。
      * 工具：sklearn中的VarianceThreshold类。
   * 特徵选择的三类方法
      * 过滤法
         * 按照特徵的发散性或者相关性指标对各个特徵进行评分，设定评分阈值或者待选择阈值的个数，选择合适的特徵。
         * 统计学指标：
            1.特徵的方差。
            2.相关係数。主要用于输出连续值的监督学习算法中。分别计算所有训练集中各个特徵与输出值之间的相关係数，设定一个阈值，选择相关係数较大的部分特徵。
            3.假设检验。1）卡方检验：可以检验某个特徵分佈和输出值分佈之间的相关性；2）F检验；3）t检验。
            4.互信息。从信息熵的角度分析各个特徵和输出值之间的关係评分。互信息值越大，説明该特徵和输出值之间的相关性越大，越需要保留。
         * 在没有思路时，可以优先使用卡方检验和互信息来做特徵选择。
      * 包装法
         * 根据目标函数，通常是预测效果评分，每次选择部分特征，或者排除部分特征。
         * 最常用的包装法是递归消除特征法(recursive feature elimination,以下简称RFE)。递归消除特征法使用一个机器学习模型来进行多轮训练，每轮训练后，消除若干权值系数的对应的特征，再基于新的特征集进行下一轮训练。
      * 嵌入法
         * 先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小来选择特征。
         * 和RFE的区别是它不是通过不停的筛掉特征来进行训练，而是使用的都是特征全集。
         * 类似于过滤法，但是它是通过机器学习训练来确定特征的优劣，而不是直接从特征的一些统计学指标来确定特征的优劣。
         * 最常用的是使用L1正则化和L2正则化来选择特征。正则化惩罚项越大，那么逻辑回归模型的系数就会越小。当正则化惩罚项大到一定的程度的时候，部分特征系数会变成0，当正则化惩罚项继续增大到一定程度时，所有的特征系数都会趋于0. 但是我们会发现一部分特征系数会更容易先变成0，这部分系数就是可以筛掉的。也就是说，我们选择特征系数较大的特征。常用的L1正则化和L2正则化来选择特征的基学习器是逻辑回归。
         * 此外也可以使用决策树或者GBDT。
         * 一般来说，可以得到特征系数coef或者可以得到特征重要度(feature importances)的算法才可以做为嵌入法的基学习器。

* 寻找高级特徵
   * 高级特徵可以一直寻找下去。
   * 寻找高级特征是模型优化的必要步骤之一。
   * 在第一次建立模型时可以先不寻找高级特征，得到以后基准模型后，再寻找高级特征进行优化。
   * 寻找高级特征最常用的方法有：
      * 若干项特征加和： 我们假设你希望根据每日销售额得到一周销售额的特征。你可以将最近的7天的销售额相加得到。 
      * 若干项特征之差： 假设你已经拥有每周销售额以及每月销售额两项特征，可以求一周前一月内的销售额。 
      * 若干项特征乘积： 假设你有商品价格和商品销量的特征，那么就可以得到销售额的特征。 
      * 若干项特征除商： 假设你有每个用户的销售额和购买的商品件数，那么就是得到该用户平均每件商品的销售额。
   * 寻找高级特征的方法需要你根据你的业务和模型需要而得，而不是随便的两两组合形成高级特征，这样容易导致特征爆炸，反而没有办法得到较好的模型。
   * 聚类的时候高级特征尽量少一点，分类回归的时候高级特征适度的多一点。
   
#### 特徵表达

* 缺失值处理
首先我们会看是该特征是连续值还是离散值。
如果是连续值，一般有两种选择，一是选择所有有该特征值的样本，然后取平均值，来填充缺失值，另一种是取中位数来填充缺失值。
如果是离散值，则一般会选择所有有该特征值的样本中最频繁出现的类别值，来填充缺失值。

* 特殊的特徵处理
有些特征的默认取值比较特殊，一般需要做了处理后才能用于算法。
   * 对于时间原始特征，处理方法有很多，这里只举例几种有代表性的方法。 
   1.使用连续的时间差值法，即计算出所有样本的时间到某一个未来时间之间的数值差距，这样这个差距是UTC的时间差，从而将时间特征转化为连续值。
   2.根据时间所在的年，月，日，星期几，小时数，将一个时间特征转化为若干个离散特征，这种方法在分析具有明显时间趋势的问题比较好用。
   3.权重法，即根据时间的新旧得到一个权重值。比如对于商品，三个月前购买的设置一个较低的权重，最近三天购买的设置一个中等的权重，在三个月内但是三天前的设置一个较大的权重。当然，还有其他的设置权重的方法，这个要根据要解决的问题来灵活确定。
   * 对地理特征，比如“广州市天河区XX街道XX号”，处理成离散值和连续值都是可以的。
   1.如果是处理成离散值，则需要转化为多个离散特征，比如城市名特征，区县特征，街道特征等。
   2.如果我们需要判断用户分布区域，则一般处理成连续值会比较好，这时可以将地址处理成经度和纬度的连续特征。

* 离散特徵的连续化处理
最常见的离散特征连续化的处理方法是独热编码one-hot encoding。处理方法其实比较简单，比如某特征的取值是高，中和低，那么我们就可以创建三个取值为0或者1的特征，将高编码为1,0,0这样三个特征，中编码为0,1,0这样三个特征，低编码为0,0,1这样三个特征。也就是说，之前的一个特征被我们转化为了三个特征。
第二个方法是特征嵌入embedding。这个一般用于深度学习中。比如对于用户的ID这个特征，如果要使用独热编码，则维度会爆炸，如果使用特征嵌入就维度低很多了。对于每个要嵌入的特征，我们会有一个特征嵌入矩阵，这个矩阵的行很大，对应我们该特征的数目。比如用户ID，如果有100万个，那么嵌入的特征矩阵的行就是100万。但是列一般比较小，比如可以取20。这样每个用户ID就转化为了一个20维的特征向量。进而参与深度学习模型。在tensorflow中，我们可以先随机初始化一个特征嵌入矩阵，对于每个用户，可以用tf.nn.embedding_lookup找到该用户的特征嵌入向量。特征嵌入矩阵会在反向传播的迭代中优化。
此外，在自然语言处理中，我们也可以用word2vec将词转化为词向量，进而可以进行一些连续值的后继处理。

* 离散特徵的离散化处理
对于原始的离散值特征，最常用的方法也是独热编码。
第二种方法是虚拟编码dummy coding，它和独热编码类似，但是它的特点是，如果我们的特征有N个取值，它只需要N-1个新的0,1特征来代替，而独热编码会用N个新特征代替。比如一个特征的取值是高，中和低，那么我们只需要两位编码，比如只编码中和低，如果是1，0则是中，0,1则是低。0,0则是高了。目前虚拟编码使用的没有独热编码广，因此一般有需要的话还是使用独热编码比较好。
有时候我们可以对特征进行研究后做一个更好的处理。比如，我们研究商品的销量对应的特征。里面有一个原始特征是季节春夏秋冬。我们可以将其转化为淡季和旺季这样的二值特征，方便建模。当然有时候转化为三值特征或者四值特征也是可以的。
对于分类问题的特征输出，我们一般需要用sklearn的LabelEncoder将其转化为0,1,2，...这样的类别标签值。

* 连续特徵的离散化处理
对于连续特征，有时候我们也可以将其做离散化处理。这样特征变得高维稀疏，方便一些算法的处理。
对常用的方法是根据阈值进行分组，比如我们根据连续值特征的分位数，将该特征分为高，中和低三个特征。将分位数从0-0.3的设置为高，0.3-0.7的设置为中，0.7-1的设置为高。
还有高级一些的方法。比如使用GBDT。在LR+GBDT的经典模型中，就是使用GDBT来先将连续值转化为离散值。那么如何转化呢？比如我们用训练集的所有连续值和标签输出来训练GBDT，最后得到的GBDT模型有两颗决策树，第一颗决策树有三个叶子节点，第二颗决策树有4个叶子节点。如果某一个样本在第一颗决策树会落在第二个叶子节点，在第二颗决策树落在第4颗叶子节点，那么它的编码就是0,1,0,0,0,0,1，一共七个离散特征，其中会有两个取值为1的位置，分别对应每颗决策树中样本落点的位置。

#### 特徵预处理

* 特徵的标准化和归一化
1.z-score标准化：
   * 最常见的特征预处理方式，基本所有的线性模型在拟合的时候都会做 z-score标准化。
   * 具体的方法是求出样本特征x的均值mean和标准差std，然后用（x-mean)/std来代替原特征。这样特征就变成了均值为0，方差为1了。   
2.max-min标准化：
   * 也称为离差标准化，预处理后使特征值映射到[0,1]之间。
   * 具体的方法是求出样本特征x的最大值max和最小值min，然后用(x-min)/(max-min)来代替原特征。
   * 如果我们希望将数据映射到任意一个区间[a,b]，而不是[0,1]，那么也很简单。用(x-min)(b-a)/(max-min)+a来代替原特征即可。
   * 实际算法中， 除非你对特征的取值区间有需求，否则max-min标准化没有 z-score标准化好用。
3.L1/L2范数标准化：
   * 如果我们只是为了统一量纲，那么通过L2范数整体标准化也是可以的。
   * 具体方法是求出每个样本特征向量的L2范数,然后用原样本特征除以L2范数代替原样本特征即可。
   * 当然L1范数标准化也是可以的，即用原样本特征除以L1范数代替原样本特征。
   * 通常情况下，范数标准化首选L2范数标准化。
4.中心化：
   * 主要是在PCA降维时，求出特征x的平均值mean后用x-mean代替原特征，也就是特征的均值变成了0, 但方差并不改变。因为PCA就是依赖方差来降维的。

* 异常特徵样本清洗
如果我们没有专业知识，如何筛选出这些异常特征样本呢？常用的方法有两种。
第一种是聚类，比如我们可以用KMeans聚类将训练样本分成若干个簇，如果某一个簇里的样本数很少，而且簇质心和其他所有的簇都很远，那么这个簇里面的样本极有可能是异常特征样本了。我们可以将其从训练集过滤掉。
第二种是异常点检测方法，主要是使用iForest或者one class SVM，使用异常点检测的机器学习算法来过滤所有的异常点。
当然，某些筛选出来的异常样本是否真的是不需要的异常特征样本，最好找懂业务的再确认一下，防止我们将正常的样本过滤掉了。

* 处理不平衡数据
我们做分类算法训练时，如果训练集里的各个类别的样本数量不是大约相同的比例，就需要处理样本不平衡问题。
一般是两种方法：权重法或者采样法。
权重法：比较简单，我们可以对训练集里的每个类别加一个权重class weight。如果该类别的样本数多，那么它的权重就低，反之则权重就高。如果更细致点，我们还可以对每个样本加权重sample weight，思路和类别权重也是一样，即样本数多的类别样本权重低，反之样本权重高。
如果权重法做了以后发现预测效果还不好，可以考虑采样法。
采样法：常用的也有两种思路，一种是对类别样本数多的样本做子采样, 直到子采样得到的A类样本数和B类别现有样本一致为止，这样我们就只用子采样得到的A类样本数和B类现有样本一起做训练集拟合模型。第二种思路是对类别样本数少的样本做过采样，直到过采样得到的B类别样本数加上B类别原来样本一起和A类样本数一致，最后再去拟合模型。
上述两种常用的采样法很简单，但是都有个问题，就是采样后改变了训练集的分布，可能导致泛化能力差。所以有的算法就通过其他方法来避免这个问题，比如SMOTE算法通过人工合成的方法来生成少类别的样本。方法也很简单，对于某一个缺少样本的类别，它会随机找出几个该类别的样本，再找出最靠近这些样本的若干个该类别样本，组成一个候选合成集合，然后在这个集合中不停的选择距离较近的两个样本，在这两个样本之间，比如中点，构造一个新的该类别样本。
